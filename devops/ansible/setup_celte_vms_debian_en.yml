---
- name: Configure Debian/Ubuntu VMs for Celte System (Part 1 - Base Setup)
  hosts: celte_vms
  become: yes
  vars:
    git_repo_url: https://github.com/celte-team/celte-system.git
    git_clone_dest: /opt/celte-system

  tasks:
    - name: Ensure Celte System Git repository is cloned and at the correct version (branch for base setup)
      ansible.builtin.git:
        repo: "{{ git_repo_url }}"
        dest: "{{ git_clone_dest }}"
        version: 211-client-sometimes-deletes-itself-when-changing-servers
        force: yes # Ensures the specified version is checked out

    - name: Remove any existing Kubernetes APT source lists to prevent conflicts
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/apt/sources.list.d/kubernetes.list # Common name
        - /etc/apt/sources.list.d/apt_kubernetes_io.list # Another possible name
      # You might need to identify other specific filenames if the error persists for kubernetes.io

    - name: Update APT cache and install prerequisites for Docker and LSB
      ansible.builtin.apt:
        name:
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
          - python3-pip
          - librust-vcpkg-dev
          - cmake
          - build-essential
          - pkg-config
          - libssl-dev
          - libffi-dev
          - python3-dev
          - libatlas-base-dev
          - gfortran
          - ninja-build
        state: present
        update_cache: yes # This will now run after potentially problematic k8s repos are removed

    - name: Create directory for APT GPG keys (if not exists)
      ansible.builtin.file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: Add Docker's official GPG key
      ansible.builtin.shell:
        cmd: curl -fsSL https://download.docker.com/linux/{{ ansible_facts.distribution | lower }}/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
        creates: /etc/apt/keyrings/docker.gpg
      register: add_docker_gpg_key
      changed_when: add_docker_gpg_key.rc == 0 and not ansible_check_mode

    - name: Ensure Docker GPG key has correct permissions
      ansible.builtin.file:
        path: /etc/apt/keyrings/docker.gpg
        mode: '0644'
      when: add_docker_gpg_key.changed

    - name: Get system architecture (dpkg)
      ansible.builtin.command: dpkg --print-architecture
      register: dpkg_arch
      changed_when: false
      check_mode: no

    - name: Get OS release codename
      ansible.builtin.shell:
        cmd: . /etc/os-release && echo "$VERSION_CODENAME"
      register: os_release_info
      changed_when: false
      check_mode: false

    - name: Add Docker APT repository
      ansible.builtin.apt_repository:
        repo: "deb [arch={{ dpkg_arch.stdout }} signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/{{ ansible_facts.distribution | lower }} {{ os_release_info.stdout | trim }} stable"
        state: present
        filename: docker # Creates /etc/apt/sources.list.d/docker.list

    - name: Install required packages (Docker, Git, Python libs, ACL, UFW)
      ansible.builtin.apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
          - git
          - acl
          - ufw
        state: present
        update_cache: yes # Update cache again after adding Docker repo

    - name: Clone Celte System Git repository (main branch)
      ansible.builtin.git:
        repo: "{{ git_repo_url }}"
        dest: "{{ git_clone_dest }}"
        version: main
        force: yes

    - name: Install 'colorama' Python package via APT
      ansible.builtin.apt:
        name: python3-colorama
        state: present

    - name: Start and enable Docker service
      ansible.builtin.systemd:
        name: docker
        state: started
        enabled: yes

    - name: Allow '{{ ansible_user }}' to manage Docker via socket
      ansible.builtin.acl:
        path: /var/run/docker.sock
        entity: "{{ ansible_user }}"
        etype: user
        permissions: rw
        state: present
      when: ansible_facts.services['docker.service'] is defined and ansible_facts.services['docker.service'].state == 'running'

    - name: Install UFW (ensures it is installed)
      ansible.builtin.apt:
        name: ufw
        state: present

    - name: Configure and enable UFW (firewall)
      ansible.builtin.ufw:
        state: enabled
        policy: deny

    - name: Allow necessary ports through UFW for base system
      ansible.builtin.ufw:
        rule: allow
        port: "{{ item.port }}"
        proto: "{{ item.proto | default('tcp') }}"
      loop:
        - { port: "22", proto: "tcp" } # SSH
        - { port: "6650", proto: "tcp" }
        - { port: "3000", proto: "tcp" }
        - { port: "6379", proto: "tcp" }
        - { port: "5540", proto: "tcp" }

    - name: Set VCPKG_ROOT environment variable system-wide
      ansible.builtin.lineinfile:
        path: /etc/environment
        line: 'VCPKG_ROOT=/usr/share/vcpkg'
        create: yes
        state: present

    - name: Ensure CMAKE_MAKE_PROGRAM is set for ninja
      ansible.builtin.lineinfile:
        path: /etc/environment
        line: 'CMAKE_MAKE_PROGRAM=/usr/bin/ninja'
        create: yes
        state: present

# ---------------------------------------------------------------------------
# Play 2: Kubernetes (K3s) Prerequisites
# ---------------------------------------------------------------------------
- name: Kubernetes (K3s) Prerequisites
  hosts: k3s_cluster
  become: yes
  tasks:
    - name: Disable swap
      ansible.builtin.command: swapoff -a
      when: ansible_swaptotal_mb > 0
      changed_when: true

    - name: Remove swap from /etc/fstab
      ansible.builtin.replace:
        path: /etc/fstab
        regexp: '^\s*([^#\s]+\s+[^#\s]+\s+swap\s+.*)$'
        replace: '# \1'
      when: ansible_swaptotal_mb > 0

    - name: Ensure br_netfilter module is loaded
      community.general.modprobe:
        name: br_netfilter
        state: present

    - name: Configure Kubernetes sysctl settings
      ansible.posix.sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        sysctl_file: /etc/sysctl.d/99-kubernetes-k3s.conf
        state: present
        reload: yes
      loop:
        - { name: "net.bridge.bridge-nf-call-iptables", value: "1" }
        - { name: "net.ipv4.ip_forward", value: "1" }
        - { name: "net.bridge.bridge-nf-call-ip6tables", value: "1" }

    - name: Define K3s master ports for UFW
      ansible.builtin.set_fact:
        k3s_master_specific_ports:
          - { port: "6443", proto: "tcp", comment: "K3s API Server" }
          - { port: "2379", proto: "tcp", comment: "K3s Embedded etcd client (master only)" }
          - { port: "2380", proto: "tcp", comment: "K3s Embedded etcd peer (master only)" }
          - { port: "8080", proto: "tcp", comment: "Custom Port (e.g., K3s API Server Rancher - verify if needed)" }

    - name: Define K3s common node ports for UFW
      ansible.builtin.set_fact:
        k3s_common_node_ports:
          - { port: "10250", proto: "tcp", comment: "Kubelet API (all nodes)" }
          - { port: "8472", proto: "udp", comment: "K3s Flannel VXLAN (all nodes)" }

    - name: Allow K3s required ports on Master nodes through UFW
      ansible.builtin.ufw:
        rule: allow
        port: "{{ item.port }}"
        proto: "{{ item.proto }}"
        comment: "{{ item.comment }}"
      loop: "{{ k3s_master_specific_ports + k3s_common_node_ports }}"
      when: inventory_hostname in groups['k3s_master']

    - name: Allow K3s required ports on Agent nodes through UFW
      ansible.builtin.ufw:
        rule: allow
        port: "{{ item.port }}"
        proto: "{{ item.proto }}"
        comment: "{{ item.comment }}"
      loop: "{{ k3s_common_node_ports }}"
      when: inventory_hostname in groups['k3s_agents']

    - name: Reload UFW to apply new rules
      ansible.builtin.command: ufw reload
      changed_when: true

# ---------------------------------------------------------------------------
# Play 3: Install K3s Server (Master Node)
# ---------------------------------------------------------------------------
- name: Install K3s Server (Master Node)
  hosts: k3s_master
  become: yes
  vars:
    git_repo_url: https://github.com/celte-team/celte-system.git
    git_clone_dest: /opt/celte-system
  tasks:
    - name: Ensure Celte System Git repository is cloned and at the correct version
      ansible.builtin.git:
        repo: "{{ git_repo_url }}"
        dest: "{{ git_clone_dest }}"
        version: 211-client-sometimes-deletes-itself-when-changing-servers
        force: yes

    - name: Stop and kill all existing K3s processes (if any)
      ansible.builtin.command: /usr/local/bin/k3s-killall.sh
      ignore_errors: yes
      changed_when: false

    - name: Uninstall any previous K3s installation (using official script)
      ansible.builtin.command: /usr/local/bin/k3s-uninstall.sh
      ignore_errors: yes
      register: k3s_uninstall_result
      changed_when: k3s_uninstall_result.rc == 0

    - name: Explicitly remove K3s service file (if exists)
      ansible.builtin.file:
        path: /etc/systemd/system/k3s.service
        state: absent
      notify: Reload systemd daemon # Important to notify systemd of changes

    - name: Explicitly remove K3s agent service file (if it was mistakenly on a master)
      ansible.builtin.file:
        path: /etc/systemd/system/k3s-agent.service
        state: absent
      notify: Reload systemd daemon

    - name: Explicitly remove K3s config directory (potential custom port settings)
      ansible.builtin.file:
        path: /etc/rancher/k3s
        state: absent # This removes the directory and its contents

    - name: Explicitly remove K3s data directory (more aggressive cleanup)
      ansible.builtin.file:
        path: /var/lib/rancher/k3s
        state: absent
      # Note: k3s-uninstall.sh should handle this, but being explicit for stubborn cases.
      # Be cautious if you ever have data in etcd you want to preserve through an upgrade (not applicable for a clean install).

    - name: Install k3s server (defaults to port 6443)
      ansible.builtin.shell:
        cmd: "curl -sfL https://get.k3s.io | sh -s - server --cluster-init"
      # Removed 'creates' to ensure the install script runs and can set up fresh,
      # relying on the uninstall/cleanup steps for idempotency in terms of a clean state.
      register: k3s_install_script_result
      changed_when: k3s_install_script_result.rc == 0 # Assume change if script runs successfully

    - name: Wait for k3s server API (port 6443) to be ready
      ansible.builtin.wait_for:
        host: "{{ ansible_default_ipv4.address | default(ansible_host) }}"
        port: 6443
        delay: 30
        timeout: 300
        state: started
      vars:
        ansible_python_interpreter: /usr/bin/python3

    - name: Wait for k3s server node token to be available
      ansible.builtin.wait_for:
        path: /var/lib/rancher/k3s/server/node-token
        timeout: 60
      vars:
        ansible_python_interpreter: /usr/bin/python3

    - name: Read the k3s node token from master
      ansible.builtin.slurp:
        src: /var/lib/rancher/k3s/server/node-token
      register: k3s_node_token_slurp
      changed_when: false

    - name: Set k3s join token fact for other hosts to use
      ansible.builtin.set_fact:
        k3s_join_token: "{{ k3s_node_token_slurp.content | b64decode | trim }}"
        cacheable: yes

    - name: Set k3s master IP fact
      ansible.builtin.set_fact:
        k3s_master_ip: "{{ ansible_default_ipv4.address | default(ansible_host) }}"
        cacheable: yes

    - name: DEBUG Master IP and Token after setting on master node
      ansible.builtin.debug:
        msg:
          - "DEBUG ON MASTER {{ inventory_hostname }}: Master IP set to: {{ k3s_master_ip | default('k3s_master_ip IS NOT SET') }}"
          - "DEBUG ON MASTER {{ inventory_hostname }}: Join Token set to: {{ k3s_join_token | default('k3s_join_token IS NOT SET') }}"

    - name: Add Helm's official GPG key to keyring
      ansible.builtin.shell:
        cmd: curl -fsSL https://baltocdn.com/helm/signing.asc | gpg --dearmor -o /etc/apt/keyrings/helm.gpg
        creates: /etc/apt/keyrings/helm.gpg
      register: add_helm_gpg_key
      changed_when: add_helm_gpg_key.rc == 0 and not ansible_check_mode

    - name: Ensure Helm GPG key has correct permissions
      ansible.builtin.file:
        path: /etc/apt/keyrings/helm.gpg
        mode: '0644'
      when: add_helm_gpg_key.changed

    - name: Add Helm APT repository (using signed-by)
      ansible.builtin.apt_repository:
        repo: "deb [arch={{ dpkg_arch.stdout }} signed-by=/etc/apt/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main"
        state: present
        filename: helm-stable-debian

    - name: Install Helm
      ansible.builtin.apt:
        name:
          - helm
        state: present
        update_cache: yes

  handlers:
    - name: Reload systemd daemon
      ansible.builtin.systemd:
        daemon_reload: yes

# ---------------------------------------------------------------------------
# Play 4: Install K3s Agents (Worker Nodes)
# ---------------------------------------------------------------------------
- name: Install K3s Agents (Worker Nodes)
  hosts: k3s_agents
  become: yes
  vars:
    k3s_master_node_name: "{{ groups['k3s_master'][0] }}"
  tasks:
    - name: Stop and kill all existing K3s agent processes (if any)
      ansible.builtin.command: /usr/local/bin/k3s-killall.sh
      ignore_errors: yes
      changed_when: false

    - name: Uninstall any previous K3s agent installation
      ansible.builtin.command: /usr/local/bin/k3s-agent-uninstall.sh
      ignore_errors: yes
      register: k3s_agent_uninstall_result
      changed_when: k3s_agent_uninstall_result.rc == 0

    - name: Explicitly remove K3s agent service file (if exists)
      ansible.builtin.file:
        path: /etc/systemd/system/k3s-agent.service
        state: absent
      notify: Reload systemd daemon # Agent play might need its own handler or ensure master's handler runs

    - name: Explicitly remove K3s config directory on agent (if any was created)
      ansible.builtin.file:
        path: /etc/rancher/k3s
        state: absent

    - name: Explicitly remove K3s data directory on agent
      ansible.builtin.file:
        path: /var/lib/rancher/k3s
        state: absent

    - name: Gather facts from the K3s master node to get its k3s_master_ip
      ansible.builtin.setup:
      delegate_to: "{{ k3s_master_node_name }}"
      delegate_facts: True
      run_once: true # Optimization: run this fact gathering only once per agent play batch

    - name: Read K3s join token directly from the master node
      ansible.builtin.slurp:
        src: /var/lib/rancher/k3s/server/node-token
      delegate_to: "{{ k3s_master_node_name }}"
      run_once: true # Optimization: run this file read only once per agent play batch
      register: k3s_node_token_slurp_from_master_for_agent

    - name: Set agent-specific facts for master IP and token
      ansible.builtin.set_fact:
        retrieved_k3s_master_ip: "{{ hostvars[k3s_master_node_name]['k3s_master_ip'] | default(hostvars[k3s_master_node_name]['ansible_default_ipv4']['address'] | default(k3s_master_node_name)) }}"
        retrieved_k3s_join_token: "{{ k3s_node_token_slurp_from_master_for_agent.content | b64decode | trim }}"

    - name: DEBUG Retrieved master IP and Token on agent node
      ansible.builtin.debug:
        msg:
          - "DEBUG ON AGENT {{ inventory_hostname }}: Master node name: {{ k3s_master_node_name }}"
          - "DEBUG ON AGENT {{ inventory_hostname }}: Retrieved Master IP: {{ retrieved_k3s_master_ip | default('retrieved_k3s_master_ip IS NOT SET') }}"
          - "DEBUG ON AGENT {{ inventory_hostname }}: Retrieved Join Token: {{ retrieved_k3s_join_token | default('retrieved_k3s_join_token IS NOT SET') }}"

    - name: Install k3s agent
      ansible.builtin.shell:
        cmd: "curl -sfL https://get.k3s.io | sh -s -"
      environment:
        K3S_URL: "https://{{ retrieved_k3s_master_ip }}:6443"
        K3S_TOKEN: "{{ retrieved_k3s_join_token }}"
      # Removed 'creates' for agent as well, relying on uninstall for clean state
      register: k3s_agent_install_script_result
      changed_when: k3s_agent_install_script_result.rc == 0

  handlers: # Agent play might need its own handler if master's isn't guaranteed to run or if they are separate batches
    - name: Reload systemd daemon
      ansible.builtin.systemd:
        daemon_reload: yes

# ---------------------------------------------------------------------------
# Play 5: Post-Installation (Fetch Kubeconfig)
# ---------------------------------------------------------------------------
- name: Post-Installation (Fetch Kubeconfig)
  hosts: k3s_master
  become: yes
  tasks:
    - name: Fetch kubeconfig file from k3s master
      ansible.builtin.fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: "./k3s_config-{{ inventory_hostname }}.yaml"
        flat: yes
        validate_checksum: yes

    - name: Display Kubeconfig instructions
      ansible.builtin.debug:
        msg:
          - "K3s cluster setup initiated."
          - "Kubeconfig file(s) have been fetched to your Ansible control machine (e.g., ./k3s_config-{{ groups['k3s_master'][0] }}.yaml)."
          - "To use kubectl with this cluster:"
          - "1. Ensure kubectl is installed on your local machine (the one running Ansible)."
          - "2. The fetched kubeconfig's server URL is likely 'https://127.0.0.1:6443' or 'https://<master-node-hostname>:6443'."
          - "   If accessing externally, you MUST change '127.0.0.1' or the internal hostname to the actual master IP: {{ hostvars[groups['k3s_master'][0]]['k3s_master_ip'] | default('MASTER_IP_NOT_FOUND_FOR_MSG') }}"
          - "   Verify the 'server:' line in the fetched kubeconfig file."
          - "3. Set the KUBECONFIG environment variable: export KUBECONFIG=$(pwd)/k3s_config-{{ groups['k3s_master'][0] }}.yaml"
          - "4. Test with: kubectl get nodes"
      delegate_to: localhost
      run_once: true

# ---------------------------------------------------------------------------
# Play 6: Deploy Celte Stack using Helm Chart (COMMENTED OUT)
# ... (original commented out content remains the same)
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# Play 7: Install Rancher Server (Single Node Docker Install for Test/Dev)
# ---------------------------------------------------------------------------
- name: Install Rancher Server
  hosts: rancher_server_host
  become: yes
  gather_facts: yes # Explicitly gather facts for this play
  vars:
    rancher_data_volume_path: "/opt/rancher"
    rancher_image_tag: "stable"
    rancher_container_name: "rancher-server" # Define a consistent name
  tasks:
    - name: Ensure Docker is installed and running
      ansible.builtin.service:
        name: docker
        state: started
        enabled: yes

    - name: Allow Rancher ports through UFW (if UFW is active)
      ansible.builtin.ufw:
        rule: allow
        port: "{{ item.port }}"
        proto: "{{ item.proto | default('tcp') }}"
        comment: "{{ item.comment }}"
      loop:
        - { port: "80", comment: "Rancher HTTP" }
        - { port: "443", comment: "Rancher HTTPS" }
      notify: Reload UFW if changed on Rancher host
      # This assumes UFW is managed on the rancher_server_host.
      # If UFW is not active, these tasks will be skipped by ufw module.

    - name: Stop and remove any existing Rancher container with the target name
      ansible.builtin.docker_container:
        name: "{{ rancher_container_name }}"
        state: absent
      ignore_errors: yes # Ignore if container doesn't exist

    - name: Stop and remove any old Rancher container named 'rancher' (cleanup)
      ansible.builtin.docker_container:
        name: "rancher" # Specific name from user's 'docker ps' output
        state: absent
      ignore_errors: yes

    - name: Create persistent data directory for Rancher
      ansible.builtin.file:
        path: "{{ rancher_data_volume_path }}"
        state: directory
        mode: '0755'

    - name: Run Rancher server Docker container
      ansible.builtin.docker_container:
        name: "{{ rancher_container_name }}"
        image: "rancher/rancher:{{ rancher_image_tag }}"
        state: started
        restart_policy: unless-stopped
        privileged: yes
        ports:
          - "80:80"
          - "443:443"
        volumes:
          - "{{ rancher_data_volume_path }}:/var/lib/rancher"
      register: rancher_container_status

    - name: Display Rancher server access information and next steps
      ansible.builtin.debug:
        msg:
          - "Rancher server container has been started on {{ inventory_hostname }}."
          - "Image: rancher/rancher:{{ rancher_image_tag }}"
          - "If the container started successfully, wait a few minutes for it to initialize."
          - "Then, access the Rancher UI at: https://{{ hostvars[inventory_hostname]['ansible_host'] | default(inventory_hostname) }}"
          - ""
          - "NEXT MANUAL STEPS:"
          - "1. Open the URL in your browser. Proceed past any self-signed certificate warnings."
          - "2. On first login, Rancher will provide a command to get the initial admin password (e.g., docker logs {{ rancher_container_name }} 2>&1 | grep \"Bootstrap Password\") or ask you to set one."
          - "3. Set your Rancher admin password and agree to the terms."
          - "4. Import your existing K3s Cluster: In Rancher UI -> Cluster Management -> Import Existing. Name it, click Create."
          - "5. Rancher will give you a 'kubectl apply -f <URL>' command. Run this command on your K3s MASTER node (using the k3s.yaml you fetched, or 'sudo k3s kubectl apply -f <URL>')."
          - "6. Wait for the cluster to become 'Active' in Rancher."
          - ""
          - "FOR AUTOMATED IMAGE UPDATES (after K3s import and app deployment):"
          - "Consider Rancher Fleet (Continuous Delivery)."
          - "Rancher Fleet setup:"
          - "  a. Prepare a Git 'Config Repo' for your celte-system Kubernetes manifests."
          - "  b. Update your CI pipeline to build/push the Docker image and then update the image tag in the 'Config Repo'."
          - "  c. In Rancher UI (Continuous Delivery -> Git Repos), add your 'Config Repo'."
          - "Refer to official Rancher and Fleet documentation for detailed steps."
      when: rancher_container_status is defined and rancher_container_status.changed
      delegate_to: localhost
      run_once: true

  handlers:
    - name: Reload UFW if changed on Rancher host
      ansible.builtin.command: ufw reload
      when: "'ufw' in (ansible_facts.packages | default({}))" # Safer check


# ---------------------------------------------------------------------------
# Play 8: Deploy Celte Stack using Helm CLI
# ---------------------------------------------------------------------------
- name: Deploy Celte Stack using Helm CLI
  hosts: k3s_master # This play runs on the K3s master node
  become: yes
  vars:
    git_repo_url: https://github.com/celte-team/celte-system.git
    git_clone_dest: /opt/celte-system
    custom_values_file_src: "values.yaml" # Name of your custom values file on the Ansible controller
    custom_values_file_dest: "{{ git_clone_dest }}/values.yaml" # Path to copy it on the master node
    celte_helm_chart_path: "{{ git_clone_dest }}/helm" # Path to the chart directory within the cloned repo
    celte_app_namespace: "celte"
    celte_helm_release_name: "celte-system"
  tasks:
    - name: Deploy Celte Stack using Helm CLI with custom values
      ansible.builtin.command:
        cmd: >
          helm upgrade --install {{ celte_helm_release_name }}
          {{ celte_helm_chart_path }}
          --namespace {{ celte_app_namespace }}
          --create-namespace
          -f {{ celte_helm_chart_path }}/values.yaml
        chdir: "{{ git_clone_dest }}"
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      register: helm_upgrade_install_result
      changed_when: helm_upgrade_install_result.rc == 0
      failed_when: helm_upgrade_install_result.rc != 0

    - name: Show k3s service status if Helm deployment fails
      ansible.builtin.shell: systemctl status k3s.service || journalctl -xeu k3s.service | tail -n 50
      register: k3s_status_on_helm_fail
      when: helm_upgrade_install_result is failed
      ignore_errors: yes

    - name: Print k3s service status on Helm failure
      ansible.builtin.debug:
        var: k3s_status_on_helm_fail.stdout_lines
      when: k3s_status_on_helm_fail is defined and k3s_status_on_helm_fail.stdout_lines is defined
